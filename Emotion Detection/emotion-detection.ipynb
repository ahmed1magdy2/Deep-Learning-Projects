{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nfrom keras.preprocessing.image import ImageDataGenerator\nimport cv2\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.optimizers import Adam","metadata":{"execution":{"iopub.status.busy":"2023-03-26T23:43:19.684124Z","iopub.execute_input":"2023-03-26T23:43:19.684653Z","iopub.status.idle":"2023-03-26T23:43:30.252223Z","shell.execute_reply.started":"2023-03-26T23:43:19.684599Z","shell.execute_reply":"2023-03-26T23:43:30.250668Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"image_gen = ImageDataGenerator(ImageDataGenerator())","metadata":{"execution":{"iopub.status.busy":"2023-03-26T23:43:30.255079Z","iopub.execute_input":"2023-03-26T23:43:30.256397Z","iopub.status.idle":"2023-03-26T23:43:30.262804Z","shell.execute_reply.started":"2023-03-26T23:43:30.256326Z","shell.execute_reply":"2023-03-26T23:43:30.261446Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_set = image_gen.flow_from_directory('/kaggle/input/emotion-detection-fer/train',\n                                                 target_size = (48,48),\n                                                 batch_size = 128,\n                                                 color_mode = \"grayscale\",\n                                                 class_mode = 'categorical')\ntest_set = image_gen.flow_from_directory('/kaggle/input/emotion-detection-fer/test',\n                                                 target_size = (48,48),\n                                                 batch_size = 128,\n                                                 color_mode = \"grayscale\",\n                                                 class_mode = 'categorical')","metadata":{"execution":{"iopub.status.busy":"2023-03-26T23:43:30.264547Z","iopub.execute_input":"2023-03-26T23:43:30.265001Z","iopub.status.idle":"2023-03-26T23:44:06.057256Z","shell.execute_reply.started":"2023-03-26T23:43:30.264938Z","shell.execute_reply":"2023-03-26T23:44:06.055638Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Found 28709 images belonging to 7 classes.\nFound 7178 images belonging to 7 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"train_set.class_indices","metadata":{"execution":{"iopub.status.busy":"2023-03-26T23:44:06.060155Z","iopub.execute_input":"2023-03-26T23:44:06.060625Z","iopub.status.idle":"2023-03-26T23:44:06.070698Z","shell.execute_reply.started":"2023-03-26T23:44:06.060576Z","shell.execute_reply":"2023-03-26T23:44:06.069236Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"{'angry': 0,\n 'disgusted': 1,\n 'fearful': 2,\n 'happy': 3,\n 'neutral': 4,\n 'sad': 5,\n 'surprised': 6}"},"metadata":{}}]},{"cell_type":"code","source":"model = keras.models.Sequential([\n    keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', input_shape=(48,48,1)),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(0.3),\n    \n    keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(0.3),\n    \n    keras.layers.Conv2D(filters=256, kernel_size=(3,3), activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(0.3),\n    \n    keras.layers.Conv2D(filters=512, kernel_size=(3,3), activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(0.3),\n    \n    keras.layers.Flatten(),\n    keras.layers.Dense(512,activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(rate = 0.3),\n    \n    keras.layers.Dense(256,activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(rate = 0.3),\n    \n    keras.layers.Dense(128,activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(rate = 0.3),\n    \n    keras.layers.Dense(64,activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(rate = 0.3),\n    \n    keras.layers.Dense(32,activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(rate = 0.3),\n    \n    keras.layers.Dense(7,activation='softmax')\n\n])","metadata":{"execution":{"iopub.status.busy":"2023-03-26T23:44:06.073085Z","iopub.execute_input":"2023-03-26T23:44:06.073960Z","iopub.status.idle":"2023-03-26T23:44:06.682590Z","shell.execute_reply.started":"2023-03-26T23:44:06.073883Z","shell.execute_reply":"2023-03-26T23:44:06.681430Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=Adam(learning_rate=0.001),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-03-26T23:44:46.017641Z","iopub.execute_input":"2023-03-26T23:44:46.018934Z","iopub.status.idle":"2023-03-26T23:44:46.044056Z","shell.execute_reply.started":"2023-03-26T23:44:46.018879Z","shell.execute_reply":"2023-03-26T23:44:46.042613Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"checkpoint = tf.keras.callbacks.ModelCheckpoint('model_emotion_detection.h5', \n                                                monitor='accuracy', verbose=1, \n                                                mode='max',save_best_only=True)\nearly = tf.keras.callbacks.EarlyStopping(monitor=\"accuracy\",\n                                         mode=\"max\",\n                                         restore_best_weights=True,\n                                         patience=10)\ncallbacks_list = [checkpoint,early]\n\nhistory = model.fit(train_set,\n                     validation_data=test_set,\n                     epochs=100,\n                     callbacks=callbacks_list)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T23:44:57.546878Z","iopub.execute_input":"2023-03-26T23:44:57.548481Z","iopub.status.idle":"2023-03-27T01:56:41.879973Z","shell.execute_reply.started":"2023-03-26T23:44:57.548408Z","shell.execute_reply":"2023-03-27T01:56:41.877890Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keras/preprocessing/image.py:1862: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n  \"This ImageDataGenerator specifies \"\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/100\n225/225 [==============================] - ETA: 0s - loss: 2.1904 - accuracy: 0.1905\nEpoch 1: accuracy improved from -inf to 0.19053, saving model to model_emotion_detection.h5\n225/225 [==============================] - 338s 1s/step - loss: 2.1904 - accuracy: 0.1905 - val_loss: 1.7888 - val_accuracy: 0.2817\nEpoch 2/100\n225/225 [==============================] - ETA: 0s - loss: 1.8317 - accuracy: 0.2663\nEpoch 2: accuracy improved from 0.19053 to 0.26629, saving model to model_emotion_detection.h5\n225/225 [==============================] - 273s 1s/step - loss: 1.8317 - accuracy: 0.2663 - val_loss: 1.7277 - val_accuracy: 0.2941\nEpoch 3/100\n225/225 [==============================] - ETA: 0s - loss: 1.6711 - accuracy: 0.3360\nEpoch 3: accuracy improved from 0.26629 to 0.33599, saving model to model_emotion_detection.h5\n225/225 [==============================] - 273s 1s/step - loss: 1.6711 - accuracy: 0.3360 - val_loss: 1.6500 - val_accuracy: 0.3706\nEpoch 4/100\n225/225 [==============================] - ETA: 0s - loss: 1.5454 - accuracy: 0.3944\nEpoch 4: accuracy improved from 0.33599 to 0.39444, saving model to model_emotion_detection.h5\n225/225 [==============================] - 274s 1s/step - loss: 1.5454 - accuracy: 0.3944 - val_loss: 1.4928 - val_accuracy: 0.4094\nEpoch 5/100\n225/225 [==============================] - ETA: 0s - loss: 1.4487 - accuracy: 0.4400\nEpoch 5: accuracy improved from 0.39444 to 0.44000, saving model to model_emotion_detection.h5\n225/225 [==============================] - 274s 1s/step - loss: 1.4487 - accuracy: 0.4400 - val_loss: 1.5055 - val_accuracy: 0.4319\nEpoch 6/100\n225/225 [==============================] - ETA: 0s - loss: 1.3800 - accuracy: 0.4720\nEpoch 6: accuracy improved from 0.44000 to 0.47201, saving model to model_emotion_detection.h5\n225/225 [==============================] - 273s 1s/step - loss: 1.3800 - accuracy: 0.4720 - val_loss: 1.2972 - val_accuracy: 0.5031\nEpoch 7/100\n225/225 [==============================] - ETA: 0s - loss: 1.3226 - accuracy: 0.4974\nEpoch 7: accuracy improved from 0.47201 to 0.49737, saving model to model_emotion_detection.h5\n225/225 [==============================] - 273s 1s/step - loss: 1.3226 - accuracy: 0.4974 - val_loss: 1.3431 - val_accuracy: 0.4869\nEpoch 8/100\n225/225 [==============================] - ETA: 0s - loss: 1.2915 - accuracy: 0.5136\nEpoch 8: accuracy improved from 0.49737 to 0.51357, saving model to model_emotion_detection.h5\n225/225 [==============================] - 272s 1s/step - loss: 1.2915 - accuracy: 0.5136 - val_loss: 1.3098 - val_accuracy: 0.4954\nEpoch 9/100\n225/225 [==============================] - ETA: 0s - loss: 1.2594 - accuracy: 0.5259\nEpoch 9: accuracy improved from 0.51357 to 0.52593, saving model to model_emotion_detection.h5\n225/225 [==============================] - 272s 1s/step - loss: 1.2594 - accuracy: 0.5259 - val_loss: 1.2064 - val_accuracy: 0.5401\nEpoch 10/100\n225/225 [==============================] - ETA: 0s - loss: 1.2227 - accuracy: 0.5421\nEpoch 10: accuracy improved from 0.52593 to 0.54209, saving model to model_emotion_detection.h5\n225/225 [==============================] - 270s 1s/step - loss: 1.2227 - accuracy: 0.5421 - val_loss: 1.2251 - val_accuracy: 0.5333\nEpoch 11/100\n225/225 [==============================] - ETA: 0s - loss: 1.1934 - accuracy: 0.5548\nEpoch 11: accuracy improved from 0.54209 to 0.55481, saving model to model_emotion_detection.h5\n225/225 [==============================] - 271s 1s/step - loss: 1.1934 - accuracy: 0.5548 - val_loss: 1.1988 - val_accuracy: 0.5385\nEpoch 12/100\n225/225 [==============================] - ETA: 0s - loss: 1.1586 - accuracy: 0.5695\nEpoch 12: accuracy improved from 0.55481 to 0.56947, saving model to model_emotion_detection.h5\n225/225 [==============================] - 272s 1s/step - loss: 1.1586 - accuracy: 0.5695 - val_loss: 1.1548 - val_accuracy: 0.5673\nEpoch 13/100\n225/225 [==============================] - ETA: 0s - loss: 1.1361 - accuracy: 0.5826\nEpoch 13: accuracy improved from 0.56947 to 0.58264, saving model to model_emotion_detection.h5\n225/225 [==============================] - 272s 1s/step - loss: 1.1361 - accuracy: 0.5826 - val_loss: 1.1464 - val_accuracy: 0.5658\nEpoch 14/100\n225/225 [==============================] - ETA: 0s - loss: 1.1047 - accuracy: 0.5935\nEpoch 14: accuracy improved from 0.58264 to 0.59351, saving model to model_emotion_detection.h5\n225/225 [==============================] - 271s 1s/step - loss: 1.1047 - accuracy: 0.5935 - val_loss: 1.1294 - val_accuracy: 0.5787\nEpoch 15/100\n225/225 [==============================] - ETA: 0s - loss: 1.0815 - accuracy: 0.6040\nEpoch 15: accuracy improved from 0.59351 to 0.60396, saving model to model_emotion_detection.h5\n225/225 [==============================] - 280s 1s/step - loss: 1.0815 - accuracy: 0.6040 - val_loss: 1.1747 - val_accuracy: 0.5630\nEpoch 16/100\n225/225 [==============================] - ETA: 0s - loss: 1.0554 - accuracy: 0.6153\nEpoch 16: accuracy improved from 0.60396 to 0.61528, saving model to model_emotion_detection.h5\n225/225 [==============================] - 270s 1s/step - loss: 1.0554 - accuracy: 0.6153 - val_loss: 1.1178 - val_accuracy: 0.5795\nEpoch 17/100\n225/225 [==============================] - ETA: 0s - loss: 1.0273 - accuracy: 0.6262\nEpoch 17: accuracy improved from 0.61528 to 0.62621, saving model to model_emotion_detection.h5\n225/225 [==============================] - 271s 1s/step - loss: 1.0273 - accuracy: 0.6262 - val_loss: 1.1852 - val_accuracy: 0.5578\nEpoch 18/100\n225/225 [==============================] - ETA: 0s - loss: 1.0048 - accuracy: 0.6351\nEpoch 18: accuracy improved from 0.62621 to 0.63510, saving model to model_emotion_detection.h5\n225/225 [==============================] - 272s 1s/step - loss: 1.0048 - accuracy: 0.6351 - val_loss: 1.1801 - val_accuracy: 0.5616\nEpoch 19/100\n225/225 [==============================] - ETA: 0s - loss: 0.9811 - accuracy: 0.6472\nEpoch 19: accuracy improved from 0.63510 to 0.64718, saving model to model_emotion_detection.h5\n225/225 [==============================] - 271s 1s/step - loss: 0.9811 - accuracy: 0.6472 - val_loss: 1.0783 - val_accuracy: 0.6039\nEpoch 20/100\n225/225 [==============================] - ETA: 0s - loss: 0.9561 - accuracy: 0.6555\nEpoch 20: accuracy improved from 0.64718 to 0.65551, saving model to model_emotion_detection.h5\n225/225 [==============================] - 271s 1s/step - loss: 0.9561 - accuracy: 0.6555 - val_loss: 1.0833 - val_accuracy: 0.5971\nEpoch 21/100\n225/225 [==============================] - ETA: 0s - loss: 0.9350 - accuracy: 0.6663\nEpoch 21: accuracy improved from 0.65551 to 0.66631, saving model to model_emotion_detection.h5\n225/225 [==============================] - 270s 1s/step - loss: 0.9350 - accuracy: 0.6663 - val_loss: 1.1092 - val_accuracy: 0.5981\nEpoch 22/100\n225/225 [==============================] - ETA: 0s - loss: 0.9029 - accuracy: 0.6779\nEpoch 22: accuracy improved from 0.66631 to 0.67787, saving model to model_emotion_detection.h5\n225/225 [==============================] - 271s 1s/step - loss: 0.9029 - accuracy: 0.6779 - val_loss: 1.0631 - val_accuracy: 0.6155\nEpoch 23/100\n225/225 [==============================] - ETA: 0s - loss: 0.9004 - accuracy: 0.6777\nEpoch 23: accuracy did not improve from 0.67787\n225/225 [==============================] - 271s 1s/step - loss: 0.9004 - accuracy: 0.6777 - val_loss: 1.0650 - val_accuracy: 0.6110\nEpoch 24/100\n225/225 [==============================] - ETA: 0s - loss: 0.8628 - accuracy: 0.6971\nEpoch 24: accuracy improved from 0.67787 to 0.69713, saving model to model_emotion_detection.h5\n225/225 [==============================] - 270s 1s/step - loss: 0.8628 - accuracy: 0.6971 - val_loss: 1.0622 - val_accuracy: 0.6133\nEpoch 25/100\n225/225 [==============================] - ETA: 0s - loss: 0.8368 - accuracy: 0.7030\nEpoch 25: accuracy improved from 0.69713 to 0.70302, saving model to model_emotion_detection.h5\n225/225 [==============================] - 269s 1s/step - loss: 0.8368 - accuracy: 0.7030 - val_loss: 1.0843 - val_accuracy: 0.6096\nEpoch 26/100\n225/225 [==============================] - ETA: 0s - loss: 0.8184 - accuracy: 0.7082\nEpoch 26: accuracy improved from 0.70302 to 0.70818, saving model to model_emotion_detection.h5\n225/225 [==============================] - 270s 1s/step - loss: 0.8184 - accuracy: 0.7082 - val_loss: 1.1399 - val_accuracy: 0.5985\nEpoch 27/100\n225/225 [==============================] - ETA: 0s - loss: 0.7901 - accuracy: 0.7204\nEpoch 27: accuracy improved from 0.70818 to 0.72040, saving model to model_emotion_detection.h5\n225/225 [==============================] - 269s 1s/step - loss: 0.7901 - accuracy: 0.7204 - val_loss: 1.1481 - val_accuracy: 0.6023\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/1085660180.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m                      \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                      \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                      callbacks=callbacks_list)\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1636\u001b[0m             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1637\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1638\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1639\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1640\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"model.evaluate(test_set)","metadata":{"execution":{"iopub.status.busy":"2023-03-27T01:57:05.350507Z","iopub.execute_input":"2023-03-27T01:57:05.351849Z","iopub.status.idle":"2023-03-27T01:57:26.034275Z","shell.execute_reply.started":"2023-03-27T01:57:05.351781Z","shell.execute_reply":"2023-03-27T01:57:26.032734Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"57/57 [==============================] - 12s 205ms/step - loss: 1.1481 - accuracy: 0.6023\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"[1.148112177848816, 0.6022568941116333]"},"metadata":{}}]},{"cell_type":"code","source":"def getlabel(x):\n    for i in train_set.class_indices:\n        if train_set.class_indices[i] == x:\n            return i","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_image(\"/kaggle/input/images/image.jpg\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = \"/kaggle/input/emotion-detection-fer/test/angry/im0.png\"\nimg = image.load_img(path)\nimg = np.expand_dims(img, axis=0)\nmodel.predict(img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = [i: i in train_set.class_indices]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_image(path):\n    img = image.load_img(path)\n    img = np.expand_dims(img, axis=0)\n    p = model.predict(img)\n    pred = [np.argmax(element) for element in p]\n    print (plt.imshow(cv2.imread(path)))\n    return labels[pred[0]]","metadata":{},"execution_count":null,"outputs":[]}]}